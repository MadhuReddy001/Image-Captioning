{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MadhuReddy001/Image-Captioning/blob/main/Spam_filter_for_Quora_questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZ2p_bszfqV0",
        "outputId": "ec3c9794-45af-4bc7-8c99-12aef1204ac9",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.8.0\n",
            "  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (24.3.25)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.12.1)\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8.0)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.17.0)\n",
            "Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8.0)\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8.0)\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8.0)\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.68.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.0) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.27.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8.0)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.32.3)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8.0)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8.0)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2.2)\n",
            "Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tf-estimator-nightly, tensorboard-plugin-wit, keras, tensorboard-data-server, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.5.0\n",
            "    Uninstalling keras-3.5.0:\n",
            "      Successfully uninstalled keras-3.5.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.25.0 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow==2.8.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa3SBtPBQrnh",
        "outputId": "c097cbff-288b-4d9a-8c7e-da4cd142f595",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.6.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install protobuf==3.20.*"
      ],
      "metadata": {
        "id": "bhu59wKtG5qd",
        "outputId": "76b35d19-41ad-4419-be04-ff61101ee491",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting protobuf==3.20.*\n",
            "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
            "Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\n",
            "pandas-gbq 0.25.0 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "fd0f8936e5184a70bd0837d33ca857d6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ4-xnugf3LS",
        "outputId": "34ae871c-3341-4783-9f02-84d6364336e7",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-06 17:57:36--  https://www.dropbox.com/sh/kpf9z73woodfssv/AAAw1_JIzpuVvwteJCma0xMla?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.dropbox.com/scl/fo/6cbk5ktsjhcd9kqxn01qj/AGuGEKF2SeUhJwGC4ju98vU?rlkey=25axzya0eum5h4qv91hpiies4&dl=0 [following]\n",
            "--2025-01-06 17:57:36--  https://www.dropbox.com/scl/fo/6cbk5ktsjhcd9kqxn01qj/AGuGEKF2SeUhJwGC4ju98vU?rlkey=25axzya0eum5h4qv91hpiies4&dl=0\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucdb4f76c379965c1bb2b8ad28df.dl.dropboxusercontent.com/zip_download_get/CFGRtVIv2TazGR2s1nnUoOmw3q1J6NTi8Qru6uJgO6Z0YFy50avoMW0vyC_YwiqNDBGQa1A4EjyloosBXiYhQg5oFtWPG1pSep23zdMmL7eJCg# [following]\n",
            "--2025-01-06 17:57:37--  https://ucdb4f76c379965c1bb2b8ad28df.dl.dropboxusercontent.com/zip_download_get/CFGRtVIv2TazGR2s1nnUoOmw3q1J6NTi8Qru6uJgO6Z0YFy50avoMW0vyC_YwiqNDBGQa1A4EjyloosBXiYhQg5oFtWPG1pSep23zdMmL7eJCg\n",
            "Resolving ucdb4f76c379965c1bb2b8ad28df.dl.dropboxusercontent.com (ucdb4f76c379965c1bb2b8ad28df.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to ucdb4f76c379965c1bb2b8ad28df.dl.dropboxusercontent.com (ucdb4f76c379965c1bb2b8ad28df.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 124207018 (118M) [application/zip]\n",
            "Saving to: ‘AAAw1_JIzpuVvwteJCma0xMla?dl=0’\n",
            "\n",
            "AAAw1_JIzpuVvwteJCm 100%[===================>] 118.45M  62.2MB/s    in 1.9s    \n",
            "\n",
            "2025-01-06 17:57:39 (62.2 MB/s) - ‘AAAw1_JIzpuVvwteJCma0xMla?dl=0’ saved [124207018/124207018]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.dropbox.com/sh/kpf9z73woodfssv/AAAw1_JIzpuVvwteJCma0xMla?dl=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ovw-qz0qiBA_",
        "outputId": "b84df9f6-2d7f-445c-d2ae-be3cbe6a9c05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim.downloader as api\n",
        "import nltk\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dropout, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk import word_tokenize\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TDNnxNgSggxF"
      },
      "outputs": [],
      "source": [
        "filter = pd.read_csv(r'/content/AAAw1_JIzpuVvwteJCma0xMla?dl=0', sep=',', encoding='latin-1', header=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7Se80ZwxuRi5"
      },
      "outputs": [],
      "source": [
        "filter.drop(index=[1306122, 1306123], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JEgsZzSvkVct"
      },
      "outputs": [],
      "source": [
        "filter.drop(columns=['\u000eBN'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
        "embedding_index = {}\n",
        "for word in word2vec_model.key_to_index:\n",
        "    embedding_index[word] = word2vec_model[word]"
      ],
      "metadata": {
        "id": "oNuwGYnxHSqZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9tS36uPPcS4R"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(r'/content/AAAw1_JIzpuVvwteJCma0xMla?dl=0', sep=',', encoding='latin-1', header=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['question_text']\n",
        "Y = df['target']"
      ],
      "metadata": {
        "id": "OnCCQTrwuT4_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Lw9LUP13csFN"
      },
      "outputs": [],
      "source": [
        "x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RAzg_oYueAXZ"
      },
      "outputs": [],
      "source": [
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train.dropna().astype(int)),\n",
        "    y=y_train.dropna().astype(int)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ifFH0CoNgull"
      },
      "outputs": [],
      "source": [
        "class_weight_dict = dict(enumerate(class_weights))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yEUkbXrHRs1m"
      },
      "outputs": [],
      "source": [
        "sent_lens = [len(word_tokenize(str(x))) for x in x_train]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Hj0652xHfs05"
      },
      "outputs": [],
      "source": [
        "max_len = 44"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tk = Tokenizer(char_level=False, split=' ')"
      ],
      "metadata": {
        "id": "k5HVXFzvvE0S"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype(str)\n",
        "x_test = x_test.astype(str)\n",
        "\n",
        "x_train = x_train.apply(lambda x: x.lower() if isinstance(x, str) else '')\n",
        "x_test = x_test.apply(lambda x: x.lower() if isinstance(x, str) else '')"
      ],
      "metadata": {
        "id": "LPNlukWQvE3y"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tk.fit_on_texts(x_train)"
      ],
      "metadata": {
        "id": "lln8YYaivE7S"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_train = tk.texts_to_sequences(x_train)\n",
        "seq_test = tk.texts_to_sequences(x_test)"
      ],
      "metadata": {
        "id": "PWM4nsx_vE-1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tk.word_index)"
      ],
      "metadata": {
        "id": "k80J7doNvFCO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_train_matrix = sequence.pad_sequences(seq_train, maxlen=max_len)\n",
        "seq_test_matrix = sequence.pad_sequences(seq_test, maxlen=max_len)"
      ],
      "metadata": {
        "id": "id22Q3G9vFo3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model = api.load(\"word2vec-google-news-300\")"
      ],
      "metadata": {
        "id": "BKdw4CeNlVrq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 300\n",
        "vocab_size = len(tk.word_index) + 1"
      ],
      "metadata": {
        "id": "T1uI-fN5vczI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in tk.word_index.items():\n",
        "    if word in word2vec_model:\n",
        "        embedding_matrix[i] = word2vec_model[word]"
      ],
      "metadata": {
        "id": "Rpt0tRNyvc2_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "T8os22RFswtD"
      },
      "outputs": [],
      "source": [
        "inputs = Input(name='text_input', shape=[max_len])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed = Embedding(len(embedding_matrix), 300,\n",
        "                  input_length=max_len, mask_zero=True,\n",
        "                  weights=[embedding_matrix], trainable=False)(inputs)"
      ],
      "metadata": {
        "id": "EfSgtmW-vtDn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_layer1 = LSTM(512)(embed)\n",
        "drop1 = Dropout(0.2)(lstm_layer1)"
      ],
      "metadata": {
        "id": "z5-AuaQVvtH0"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_layer2 = Dense(256, activation='relu')(drop1)\n",
        "drop2 = Dropout(0.2)(lstm_layer2)"
      ],
      "metadata": {
        "id": "_iCzd4GPvtLZ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_layer3 = Dense(128, activation='relu')(drop2)\n",
        "drop3 = Dropout(0.2)(lstm_layer3)"
      ],
      "metadata": {
        "id": "WsooWbMMvtO5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense1 = Dense(25, activation='relu')(drop3)\n",
        "drop4 = Dropout(0.2)(dense1)"
      ],
      "metadata": {
        "id": "iXWuUc9-vtSK"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_layer = Dense(1, activation='sigmoid')(drop4)"
      ],
      "metadata": {
        "id": "mbP5-Y73vtVQ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs=inputs, outputs=final_layer)"
      ],
      "metadata": {
        "id": "BXEbJdqzufwc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "fYbr9WMy0WGj"
      },
      "outputs": [],
      "source": [
        "filepath='/content/weights-{epoch:02d}-{val_loss:.4f}.keras' # Changed .h5 to .keras\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1,\n",
        "                             save_best_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "5qVS3fMI0cAn"
      },
      "outputs": [],
      "source": [
        "earlystop = EarlyStopping(monitor='val_loss', patience=3,\n",
        "                          verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "MeFk1NL7xKEj"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 44\n",
        "EMBEDDING_DIM = 100\n",
        "vocab_size = len(tk.word_index)"
      ],
      "metadata": {
        "id": "dcfbbjRnLIID"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = Input(name='text_input', shape=[max_len])"
      ],
      "metadata": {
        "id": "wRrxHvXgv7vB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed = Embedding(vocab_size + 1, EMBEDDING_DIM,\n",
        "                  input_length=max_len, mask_zero=True,\n",
        "                  trainable=False)(inputs)"
      ],
      "metadata": {
        "id": "pwnq7RS0v7yt"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_layer1 = LSTM(512)(embed)\n",
        "drop1 = Dropout(0.2)(lstm_layer1)"
      ],
      "metadata": {
        "id": "A9Mso07Dv71_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_layer2 = Dense(256, activation='relu')(drop1)\n",
        "drop2 = Dropout(0.2)(lstm_layer2)"
      ],
      "metadata": {
        "id": "POWcGqBBv75d"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_layer3 = Dense(128, activation='relu')(drop2)\n",
        "drop3 = Dropout(0.2)(lstm_layer3)"
      ],
      "metadata": {
        "id": "mmdd21gbv79u"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense1 = Dense(25, activation='relu')(drop3)\n",
        "drop4 = Dropout(0.2)(dense1)"
      ],
      "metadata": {
        "id": "YseuX82lwFhN"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_layer = Dense(1, activation='sigmoid')(drop4)"
      ],
      "metadata": {
        "id": "fUVMlPTawFkR"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs=inputs, outputs=final_layer)"
      ],
      "metadata": {
        "id": "X9NDjKegu21Q"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_nan_indices = ~y_train.isna()\n",
        "seq_train_matrix = seq_train_matrix[non_nan_indices]\n",
        "y_train = y_train[non_nan_indices]"
      ],
      "metadata": {
        "id": "3uWynna_KyjX"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ros = RandomOverSampler(random_state=42)\n",
        "seq_train_matrix_resampled, y_train_resampled = ros.fit_resample(\n",
        "    seq_train_matrix, y_train\n",
        ")"
      ],
      "metadata": {
        "id": "Qk2ndtVWu5CL",
        "outputId": "79a012d9-cc33-4596-9c39-d1fa657d24ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Scpw-OhTukJ4"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    seq_train_matrix_resampled,\n",
        "    y_train_resampled,\n",
        "    epochs=15,\n",
        "    batch_size=1000,\n",
        "    validation_data=(seq_test_matrix, y_test),\n",
        "    callbacks=[earlystop, checkpoint]\n",
        ")"
      ],
      "metadata": {
        "id": "CnRezxLU1mXE",
        "outputId": "408f4432-1808-47e1-89c6-85f5e4de8800",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mN9KQx2wxwXJ"
      },
      "outputs": [],
      "source": [
        "best_model = load_model('/content/weights-04-0.2007.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CljDfAt7--w3"
      },
      "outputs": [],
      "source": [
        "best_model_weights = load_model('/content/weights-04-0.2007.keras').get_weights()\n",
        "\n",
        "inputs = Input(name='text_input', shape=[max_len])\n",
        "embed = Embedding(vocab_size + 1, 300, input_length=max_len, mask_zero=True,\n",
        "                  weights=[embedding_matrix], trainable=False)(inputs)\n",
        "lstm_layer1 = LSTM(512, use_cudnn=False)(embed)\n",
        "drop1 = Dropout(0.2)(lstm_layer1)\n",
        "lstm_layer2 = Dense(256, activation='relu')(drop1)\n",
        "drop2 = Dropout(0.2)(lstm_layer2)\n",
        "lstm_layer3 = Dense(128, activation='relu')(drop2)\n",
        "drop3 = Dropout(0.2)(lstm_layer3)\n",
        "dense1 = Dense(25, activation='relu')(lstm_layer3)\n",
        "drop4 = Dropout(0.2)(dense1)\n",
        "final_layer = Dense(1, activation='sigmoid')(drop4)\n",
        "\n",
        "recreated_model = Model(inputs=inputs, outputs=final_layer)\n",
        "\n",
        "recreated_model.set_weights(best_model_weights)\n",
        "\n",
        "p = recreated_model.predict(seq_test_matrix)\n",
        "roc_auc_score(y_test, p)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}